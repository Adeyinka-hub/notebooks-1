{"cells":[{"metadata":{"_uuid":"52f73d2152890a8a5194ce0a28b4d2daf71369c5"},"cell_type":"markdown","source":"# Exploratory Data Analysis + Features"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom math import pi\n\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nsns.set(style=\"white\", color_codes=True)\n%matplotlib inline \nimport scipy\nfrom scipy.stats import describe\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport sys\nfrom fastai.structured import *\nfrom fastai.column_data import *\nfrom sklearn.model_selection import *\n\ntrain = pd.read_csv(\"../input/train.csv\") # the train dataset is now a Pandas DataFrame\ntest = pd.read_csv(\"../input/test.csv\") # the train dataset is now a Pandas DataFrame\n\n# Let's see what's in the trainings data - Jupyter notebooks print the result of the last thing you do\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b048974b427251a3ab040f29c73c200d927c6f98"},"cell_type":"markdown","source":"## Shape of the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(\"Santander Value Prediction Challenge train -  rows:\",train.shape[0],\" columns:\", train.shape[1])\nprint(\"Santander Value Prediction Challenge test -  rows:\",test.shape[0],\" columns:\", test.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd4e63261d3b2523823377a8a870f2e6853f5f09"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1826b422ede74f3478ba116e0e206865beac5f48"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd2568b5a665e3d8d7747c743b05c434da757c7d"},"cell_type":"markdown","source":"## Missing values"},{"metadata":{"trusted":true,"_uuid":"24bf2412f95419761ce94f805aa5253bc728b8ad"},"cell_type":"code","source":"train.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f641cd9b3feaedd12d67af4f7592c3ceb43a6251"},"cell_type":"code","source":"test.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e90627556238b3d3bfe7f9e6daaa3d307351eb9"},"cell_type":"markdown","source":"## Types of Feature"},{"metadata":{"trusted":true,"_uuid":"cf1891e08cc5b007e5bdc5deb5351e1e6f3ea6f3"},"cell_type":"code","source":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed51735aeae356a75bedb5ec17e5171e3cd102f0"},"cell_type":"markdown","source":"## Distribution of Target Variable"},{"metadata":{"trusted":true,"_uuid":"b4b9763712b5ca776cf25114094187c771e91128"},"cell_type":"code","source":"plt.title(\"Distribution of Target\")\nsns.distplot(train['target'].dropna(),color='blue', kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"129cac529f65d8ae4f284fbb4d199be47f1d4238"},"cell_type":"markdown","source":"### Violin distribution of target"},{"metadata":{"trusted":true,"_uuid":"d6a112f91ffaa56f06e237e37895b7758959b02e"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train.target.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53bf1f94234589ac4c67b25cf92f89c9c8c8ef28"},"cell_type":"code","source":"plt.title(\"Distribution of log(target)\")\nsns.distplot(np.log1p(train['target']).dropna(),color='blue', kde=True,bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"920929b971d05354ecb9e50bd139da6df6e2cf7a"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(1+train.target.values))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ac8eaedbb6fe2516fa85064bedf3bfb348d72ce"},"cell_type":"markdown","source":"## Identifying features that are highly correlated with target"},{"metadata":{"trusted":true,"_uuid":"4295a38e1eb25c542014b0f5fa485449adc43de8"},"cell_type":"code","source":"labels = []\nvalues = []\nfor col in train.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(np.corrcoef(train[col].values, train[\"target\"].values)[0,1])\ncorr_df = pd.DataFrame({'columns_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.25) | (corr_df['corr_values']<-0.25)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,6))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='black')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.columns_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd9a9ace93e2545bd2e12c6729cedc0095033998"},"cell_type":"markdown","source":"## Correlation matrix of the most highly correlated features"},{"metadata":{"trusted":true,"_uuid":"e67c4440eed7d4400ba8d1cdc6f30acab90e134a"},"cell_type":"code","source":"temp_df = train[corr_df.columns_labels.tolist()]\ncorrmat = temp_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(corrmat, vmax=1., square=True, cmap=plt.cm.BrBG)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b785e525eaead3136c688b7662b74bf3edd4949c"},"cell_type":"markdown","source":"## Sparsity"},{"metadata":{"trusted":true,"_uuid":"7dc0ee0e099f3d1bf9bb6278519ca260ed73dd06"},"cell_type":"code","source":"sparsity = {\n    col: (train[col] == 0).mean()\n    for idx, col in enumerate(train)\n}\nsparsity = pd.Series(sparsity)\n\nfig = plt.figure(figsize=[7,12])\nax = fig.add_subplot(211)\nax.hist(sparsity, range=(0,1), bins=100)\nax.set_xlabel('Sparsity of Features')\nax.set_ylabel('Number of Features')\nax = fig.add_subplot(212)\nax.hist(sparsity, range=(0.8,1), bins=100)\nax.set_xlabel('Sparsity of Features')\nax.set_ylabel('Number of Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d052768ec81d6a0845743749c183ec449d500e85","collapsed":true},"cell_type":"code","source":"cat_flds = []\nbs = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bd2be34acbeb1559735a191d9e3401187558711","collapsed":true},"cell_type":"code","source":"test = test.set_index('ID')\ntrain['target'] = np.log(train['target'])\nx, y, nas = proc_df(train, 'target', skip_flds=['ID'])\ndf_train_x, df_val_x, df_train_y, df_val_y= train_test_split(x, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c888c93a89b26346a6561232ac3e9127ddb964b3","collapsed":true},"cell_type":"code","source":"model_data = ColumnarModelData.from_data_frames(\n    '.', df_train_x, df_val_x, df_train_y, df_val_y, cat_flds, bs, is_reg=True, is_multi=False, test_df=test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8438a9f4ed4cbf6c192dc93a709a6130c29c2ef"},"cell_type":"code","source":"emb_szs = []\nn_cont = len(df_train_x.columns)\nemb_drop = 0.0\nout_sz = 1\nszs = [400, 50]\ndrops = [0.0,0.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b7067e1b338758cf9bd2a630173aeefab533ba29"},"cell_type":"code","source":"learner = model_data.get_learner(emb_szs, n_cont, emb_drop, out_sz, szs, drops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb541f4aa354834bb49e86082dcf21ea0a6349b"},"cell_type":"code","source":"learner.lr_find2(start_lr=1, end_lr=1000, num_it=500)\nlearner.sched.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b65037e03cdd696a1722bdac843a70a90e7e3287"},"cell_type":"code","source":"learner.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aca8e8e9fa1ef0ad3b8e7b1165669202285b9a85"},"cell_type":"code","source":"lr = 0.105\nlearner.fit(lr, 10, cycle_len=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f6576f698260b9f2b91ce26299fadd87e215d632"},"cell_type":"code","source":"preds = learner.predict(is_test = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3dd4a7ac1d477fb0cc642c296daef44c2d6843b6"},"cell_type":"code","source":"train_describe = train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fee262fc85771f7a47da74f0bbac3ab7d1a9286"},"cell_type":"code","source":"train_describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"be924d65c161ad7a81822403925822e21c32bc1b"},"cell_type":"code","source":"test_describe = test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d21d017e09a8cdb949e00fc64038247b10e4097f"},"cell_type":"code","source":"test_describe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db59bd56490af57f63ec76529e457e5974e0d3fb"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(train.target.values, bins=100)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45fafb825623c193529dd00601f0fb35dd556bd8"},"cell_type":"code","source":"plt.figure(figsize=(30, 5))\nx = train.iloc[1]\nplt.hist(x)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Log 1+Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae266c8bcbe263f799710d3edb19305df8b02bb8"},"cell_type":"markdown","source":"*This is a highly skewed distribution, so let's try to re-plot it with with log transform of the target.*\n\n"},{"metadata":{"trusted":true,"_uuid":"781b225e4cd17bae5d0641599677d8cecc1aa8d2"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(np.log(1+train.target.values), bins=100)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Log 1+Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fa2e72b49e9d0e17e9307de8feff4c4f73f5612"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(1+train.target.values))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"068c9451f4b26fb70ad2dcb2492786c22ac15746"},"cell_type":"markdown","source":"*Let's take a look at the statistics of the Log(1+target)*"},{"metadata":{"trusted":true,"_uuid":"b7ff5088cf86eb0ccca414e9c4ce3278af0fe3f2"},"cell_type":"code","source":"train_log_target = train[['target']]\ntrain_log_target['target'] = np.log(1+train['target'].values)\ntrain_log_target.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc6227f00ca5b173fc38b00c2be5989bfc7b671c"},"cell_type":"markdown","source":"*We see that the statistical properties of teh Log(1+Target) distribution are much more amenable.*\n\n*Now let's take a look at columns with constant value.*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"753c703c5930488c5d0f7aeddb3611aec3029b7f"},"cell_type":"code","source":"constant_train = train.loc[:, (train == train.iloc[0]).all()].columns.tolist()\nconstant_test = test.loc[:, (test == test.iloc[0]).all()].columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cf03f2808f10e29f747fa1f364891af5f027e17"},"cell_type":"code","source":"print('Number of constant columns in the train set:', len(constant_train))\nprint('Number of constant columns in the test set:', len(constant_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d955c514bbe61d2fdbd715d2d6f4db1c76cb39c3"},"cell_type":"markdown","source":"> So this is interesting: there are 256 constant columns in the train set, but none in the test set. These constant columns are thus most likely an artifact of the way that the train and test sets were constructed, and not necessarily irrelevant in their own right. This is yet another byproduct of having a very small dataset. For most problems it would be useful to take a look at the description of these columns, but in this competition they are anonymized, and thus would not yield any useful information.\n\nSo let's subset the colums that we'd use to just those that are not constant."},{"metadata":{"trusted":true,"_uuid":"0517765c02326ebc8069ab34b4c96d152df90158"},"cell_type":"code","source":"columns_to_use = test.columns.tolist()\ndel columns_to_use[0] # Remove 'ID'\ncolumns_to_use = [x for x in columns_to_use if x not in constant_train] #Remove all 0 columns\nlen(columns_to_use)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ffa47773c7d7709d98e978dd96322c89d9dab92"},"cell_type":"markdown","source":"> So we have the total of 4735 columns to work with. However, as mentioned earlier, most of these columns seem to be filled predominatly with zeros. Let's try to get a better sense of this data."},{"metadata":{"trusted":true,"_uuid":"6bef0090f30ec22d57930376c04fdb517a136f77"},"cell_type":"code","source":"describe(train[columns_to_use].values, axis=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0782c2c2cc7b65319f1346975838421817e53fbc"},"cell_type":"markdown","source":"> If we treat all the train matrix values as if they belonged to a single row vector, we see a huge amount of varience, far exceeding the similar variance for the target variable.\n\nNow let's plot it to see how diverse the numerical values are."},{"metadata":{"trusted":true,"_uuid":"2bed3285629b7967ab88e0af6838115ae47e8cf2"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(train[columns_to_use].values.flatten(), bins=50)\nplt.title('Histogram all train counts')\nplt.xlabel('Count')\nplt.ylabel('Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f4419742240c1f636cefe238c8742d195a6b76"},"cell_type":"markdown","source":"> Most of the values are heavily concentrated around 0\nLet's see with the log plot.."},{"metadata":{"trusted":true,"_uuid":"76621062ddfcc21686cf649ea6690d3e7a259958"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(np.log(train[columns_to_use].values.flatten()+1), bins=50)\nplt.title('Log Histogram all train counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a46d60ce3196d282eb830f6169d9bbfb29e4353"},"cell_type":"markdown","source":"> Only marginal improvement - there is a verly small bump close to 15.\n\nLet's try out with violin plot"},{"metadata":{"trusted":true,"_uuid":"695e7aab2e94c7cb362e34417444672cb7fcc656"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(train[columns_to_use].values.flatten()+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b8f5268a851a83aeb863e533a7daca6a5463500"},"cell_type":"markdown","source":"*Not really - the plot looks nicer, but the overall shape is almost same.*\n\nlet's take a look at the distribution of non-zero values."},{"metadata":{"trusted":true,"_uuid":"3d8faa1f9e7f91997ea808197bc1e5b73ac9776a"},"cell_type":"code","source":"train_nz = np.log(train[columns_to_use].values.flatten()+1)\ntrain_nz = train_nz[np.nonzero(train_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(train_nz, bins=50)\nplt.title('Log Histogram nonzero train counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41dd6c6b89c94f73345914c62f5b6d9e6c9a86eb"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train_nz)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af319d6078f16471fe67a3c807c395759abbf2b7"},"cell_type":"code","source":"describe(train_nz)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b3547752139e3a2f64670f59528b76e27351e92"},"cell_type":"markdown","source":"Let's do the same thing with the test data."},{"metadata":{"trusted":true,"_uuid":"736b44229f7654c4ed2d5cee60f9b1b8d2ebe28c"},"cell_type":"code","source":"test_nz = np.log(test[columns_to_use].values.flatten()+1)\ntest_nz = test_nz[np.nonzero(test_nz)]\nplt.figure(figsize=(12, 5))\nplt.hist(test_nz, bins=50)\nplt.title('Log Histogram nonzero test counts')\nplt.xlabel('Count')\nplt.ylabel('Log value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"102302c771f354c31c9ca4073a94a303d8e122ef"},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=test_nz)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd3ddee0d392922703c62491c8a63cafc7388c1e"},"cell_type":"code","source":"describe(test_nz)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"159133b30db70df596f9bda879ba317301f6982d"},"cell_type":"markdown","source":"*Again, we see that these distributions look similar, but they are definitely not the same.*\n\nlet's take a closer look at the shape and content of the train data. We want to get a better numerical grasp of the true extent of zeros."},{"metadata":{"trusted":true,"_uuid":"d57b5e640e92bc81be5c9cfd3a4b0dda88387030"},"cell_type":"code","source":"train[columns_to_use].values.flatten().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be3af81cec23ee8643d7232679fd82dcd49f42d5"},"cell_type":"code","source":"((train[columns_to_use].values.flatten())==0).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bb0a7daefbd37cf38d0d234fc5690965c40418c"},"cell_type":"markdown","source":"*Almost 97% of all values in the train dataframe are zeros. That looks pretty sparse to me, but let's see how much variation is there between different columns.*"},{"metadata":{"trusted":true,"_uuid":"bf8585ee6d88049176f7b56df16f3ac74dde3004"},"cell_type":"code","source":"train_zeros = pd.DataFrame({'Percentile':((train[columns_to_use].values)==0).mean(axis=0),\n                           'Column' : columns_to_use})\ntrain_zeros.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0526a86591da30c55d41bb5ecd22773840b48033"},"cell_type":"code","source":"describe(train_zeros.Percentile.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12751cce0d8aff1898abcd332603033f5ea6306b"},"cell_type":"markdown","source":"*It seems that the vast majority of columns have 95+ percent of zeros in them. Let's see how would that look on a plot.*"},{"metadata":{"trusted":true,"_uuid":"da2861ac4c8fe7aaf9826395f53ec16470939f0e"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(train_zeros.Percentile.values, bins=50)\nplt.title('Histogram percentage zeros train counts')\nplt.xlabel('Count')\nplt.ylabel('Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5e88d7e0db248d0b7c2f82ee90a306aaaa43d48"},"cell_type":"code","source":"describe(np.log(train[columns_to_use].values+1), axis=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b647f8e9b7cfd57b0aa0db062753915dd098cfb1"},"cell_type":"code","source":"describe(test[columns_to_use].values, axis=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4a744a1a41b905d3089d34ed65a9ada6bc35de"},"cell_type":"code","source":"describe(np.log(test[columns_to_use].values+1), axis=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540e567aa4168ca34a535787f1440fe78e7d5b30"},"cell_type":"code","source":"test_zeros = pd.DataFrame({'Percentile':(np.log(1+test[columns_to_use].values)==0).mean(axis=0),\n                           'Column' : columns_to_use})\ntest_zeros.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a65cbd9e8c87afc30d2eda72eb6a33dce607e60"},"cell_type":"code","source":"describe(test_zeros.Percentile.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ee5de9bfb7b8ecb690485fb642d72b2633c79f"},"cell_type":"code","source":"y = np.log(1+train.target.values)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dc5028f6c3d69373a1eb46aa83deb944d5c0ae0"},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f84d8ef7ea39046a2a997f5ff3f585b13a02565b"},"cell_type":"markdown","source":"## Predictive Modeling"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"267f924a90902a73a0415476f0c98d81771197eb"},"cell_type":"code","source":"train_1 = lgb.Dataset(train[columns_to_use],y ,feature_name = \"auto\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8768c19bed529d9d919b98113db4bbaf9f443f02"},"cell_type":"code","source":"params = {'boosting_type': 'gbdt', \n          'objective': 'regression', \n          'metric': 'rmse', \n          'learning_rate': 0.0105, \n          'num_leaves': 100, \n          'feature_fraction': 0.4, \n          'bagging_fraction': 0.6, \n          'max_depth': 5, \n          'min_child_weight': 10}\n\n\nclf = lgb.train(params,\n        train_1,\n        num_boost_round = 400,\n        verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3099ad620771bf538b46a2e79d5828dbffbbaa42"},"cell_type":"code","source":"preds = clf.predict(test[columns_to_use])\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9162e5d341f72b24c6347bc34db93a70ec6f612b"},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission.target = np.exp(preds)-1\nsample_submission.to_csv('simple_lgbm.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5f9abbf252b9755f4d555814ab8fa8012ec5bff"},"cell_type":"code","source":"nr_splits = 5\nrandom_state = 1054\n\ny_oof = np.zeros((y.shape[0]))\ntotal_preds = 0\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38177ed20eb7d43d77578e1c8deb39484fb045a6"},"cell_type":"code","source":"params['max_depth'] = 4\n\ny_oof_2 = np.zeros((y.shape[0]))\ntotal_preds_2 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_2 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_2[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cea0ea16067e6b73ce611955324ad9406f6131f1"},"cell_type":"code","source":"params['max_depth'] = 6\n\ny_oof_3 = np.zeros((y.shape[0]))\ntotal_preds_3 = 0\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_3 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_3[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ef7fe245389d5f796df27e386c936a6e5a0f00"},"cell_type":"code","source":"params['max_depth'] = 7\n\ny_oof_4 = np.zeros((y.shape[0]))\ntotal_preds_4 = 0\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_4 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_4[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f55523fee97df168a10c01249d3b2b677305de0"},"cell_type":"code","source":"params['max_depth'] = 8\n\ny_oof_5 = np.zeros((y.shape[0]))\ntotal_preds_5 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_5 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_5[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40b179da24324df3eaa95c40ab300034f1417e08"},"cell_type":"code","source":"params['max_depth'] = 10\n\ny_oof_6 = np.zeros((y.shape[0]))\ntotal_preds_6 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_6 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_6[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_6)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2926598870239fc31c06537e0cbacf0d8b047a2"},"cell_type":"code","source":"params['max_depth'] = 12\n\ny_oof_7 = np.zeros((y.shape[0]))\ntotal_preds_7 = 0\n\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = lgb.Dataset(X_train,y_train ,feature_name = \"auto\")\n    val = lgb.Dataset(X_val ,y_val ,feature_name = \"auto\")\n    clf = lgb.train(params,train_1,num_boost_round = 400,verbose_eval=True)\n    \n    total_preds_7 += clf.predict(test[columns_to_use])/nr_splits\n    pred_oof = clf.predict(X_val)\n    y_oof_7[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_7)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4986ee766aee34e93a752c8141a2c381c9c53f"},"cell_type":"code","source":"print('Total error', np.sqrt(mean_squared_error(y, 1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)))\nprint('Total error', np.sqrt(mean_squared_error(y, -0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4)))\nprint('Total error', np.sqrt(mean_squared_error(y, 0.75*(1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)+\n                                                0.25*(-0.5*y_oof-0.5*y_oof_2-y_oof_3\n                                                +3*y_oof_4))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cff62f4cd500e33ea9b4208cd0b45f6836415551"},"cell_type":"code","source":"sub_preds = (0.75*(1.4*(1.6*total_preds_7-0.6*total_preds_6)-0.4*total_preds_5)+\n                                                0.25*(-0.5*total_preds-0.5*total_preds_2-total_preds_3\n                                                +3*total_preds_4))\nsample_submission.target = np.exp(sub_preds)-1\nsample_submission.to_csv('submission_1.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f345ea67e6f45a39e5ec136d3892471e6db74494"},"cell_type":"code","source":"params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.01,\n          'max_depth': 10, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n\ny_oof_8 = np.zeros((y.shape[0]))\ntotal_preds_8 = 0\n\ndtest = xgb.DMatrix(test[columns_to_use])\n\nkf = KFold(n_splits=nr_splits, shuffle=True, random_state=random_state)\nfor i, (train_index, val_index) in enumerate(kf.split(y)):\n    print('Fitting fold', i+1, 'out of', nr_splits)\n    X_train, X_val  = train[columns_to_use].iloc[train_index], train[columns_to_use].iloc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    \n    train_1 = xgb.DMatrix(X_train, y_train)\n    val = xgb.DMatrix(X_val, y_val)\n    \n    watchlist = [(train_1, 'train'), (val, 'val')]\n    \n    clf = xgb.train(params, train_1, 1000, watchlist, \n                          maximize=False, early_stopping_rounds = 60, verbose_eval=100)\n\n    \n    total_preds_8 += clf.predict(dtest, ntree_limit=clf.best_ntree_limit)/nr_splits\n    pred_oof = clf.predict(val, ntree_limit=clf.best_ntree_limit)\n    y_oof_8[val_index] = pred_oof\n    print('Fold error', np.sqrt(mean_squared_error(y_val, pred_oof)))\n\nprint('Total error', np.sqrt(mean_squared_error(y, y_oof_8)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1695b395eab694320b48e40eae4d60cb38345648"},"cell_type":"code","source":"print('Total error', np.sqrt(mean_squared_error(y, 0.7*(0.75*(1.4*(1.6*y_oof_7-0.6*y_oof_6)-0.4*y_oof_5)+0.25*(-0.5*y_oof-0.5*y_oof_2-y_oof_3+3*y_oof_4))+0.3*y_oof_8)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1099dc9a993e08b4ca393a5f5fb5bf4ad3133b6f"},"cell_type":"code","source":"sub_preds = (0.7*(0.75*(1.4*(1.6*total_preds_7-0.6*total_preds_6)-0.4*total_preds_5)+0.25*(-0.5*total_preds-0.5*total_preds_2-total_preds_3+3*total_preds_4))+0.3*total_preds_8)\n\nsample_submission.target = np.exp(sub_preds)-1\nsample_submission.to_csv('blended_submission_2.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}